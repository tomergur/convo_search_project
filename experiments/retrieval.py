import argparse
import json
import time


from pyserini.search import SimpleSearcher
import tensorflow as tf

from convo_search_project.pipeline import Pipeline
from convo_search_project.rewriters import AllHistoryRewriter, PrevUtteranceRewriter, FirstUtteranceRewriter, \
    SeparateUtterancesRewriter, T5Rewriter, QuReTeCRewriter, FileRewriter
from convo_search_project.reranker import BertReranker
#TODO: delete
from convo_search_project.mono_bert import MonoBERT

def parse_args():
    parser = argparse.ArgumentParser(argument_default=argparse.SUPPRESS,
                                     description='running retrieval pipeline for our convo search project')
    parser.add_argument('--run_name', required=True,
                        help='the name of the current run. will be the name of the output files')
    parser.add_argument('--input_queries_file', required=True, help='input queries file')
    parser.add_argument('--output_dir', required=True, help='output dir for the run')
    parser.add_argument('--count', type=int, default=1000,
                        help="number of result retrieved in the first stage retrieval")
    parser.add_argument('--tpu',help='name of tpu device')

    parser.add_argument("--use_manual_run", action='store_true',default=False,
                        help='use the manually rewritten queries')

    # Parameters for BM25.
    parser.add_argument('--k1', type=float, default=0.82, help='BM25 k1 parameter')
    parser.add_argument('--b', type=float, default=0.68, help='BM25 b parameter')

    # parameters for rewriters
    parser.add_argument('--add_prev_utter', action='store_true',
                        help='add the previous utterance for query expansion')
    parser.add_argument('--add_first_utter', action='store_true',
                        help='add the first utterance for query expansion')
    parser.add_argument('--add_all_history', action='store_true',
                        help='add all previous utterance for query expansion')
    parser.add_argument('--separate_utterances_rewriter', action='store_true',
                        help='create a different query for each utterance')

    #T5 rewriter
    parser.add_argument('--T5_rewriter', action='store_true', help='use the t5 rewriter')
    parser.add_argument('--T5_model_str',help='t5 rewriter model str')
    parser.add_argument('--T5_num_queries_generated',type=int,help='number of queries generated by the t5 model')
    # file rewriter
    parser.add_argument('--queries_rewrites_path', help='path to query rewrites cached in csv')

    # QuReTeC baseline
    parser.add_argument('--quretec', action='store_true')
    parser.add_argument('--quretec_model_path', help='model path for QuReTec')

    # parameters for reranker
    parser.add_argument('--rerank', action='store_true', default=False, help='rerank BM25 output using BERT')
    parser.add_argument('--reranker_batch_size',type=int,default=32,help='reranker batch size for inference')
    # parser.add_argument('--reranker_device', default='cuda', help='reranker device to use')

    # Return args
    args = parser.parse_args()
    return args


def output_run_file(output_path, runs):
    print("write output", output_path)
    with open(output_path, "w") as f:
        for qid, run_res in runs.items():
            for rank, doc in enumerate(run_res):
                docno = doc.docid
                score = doc.score
                f.write("{}\tQ0\t{}\t{}\t{}\t{}\n".format(qid, docno, rank + 1, score, "convo"))


def build_bert_reranker2(
        name_or_path: str = "castorini/monobert-large-msmarco-finetune-only",batch_size=32):
    """Returns a BERT reranker using the provided model name or path to load from"""
    model = BertReranker.get_model(name_or_path, from_pt=True)
    tokenizer = BertReranker.get_tokenizer(name_or_path)
    return BertReranker(model, tokenizer,batch_size=batch_size)


def build_bert_reranker(
        name_or_path: str = "castorini/monobert-large-msmarco-finetune-only",
        device: str = None):
    """Returns a BERT reranker using the provided model name or path to load from"""
    model = MonoBERT.get_model(name_or_path, device=device)
    tokenizer = MonoBERT.get_tokenizer(name_or_path)
    return MonoBERT(model, tokenizer)


def run_exp(args, pipeline):
    input_queries_file = args.input_queries_file
    query_field = "manual_rewritten_utterance" if args.use_manual_run else "raw_utterance"
    with open(input_queries_file) as json_file:
        data = json.load(json_file)
    runs = {}
    for session in data:
        start_time = time.time()
        session_num = str(session["number"])
        history = []
        for turn_id, conversations in enumerate(session["turn"]):
            query_start_time = time.time()
            query = conversations[query_field]
            conversation_num = str(conversations["number"])
            qid = session_num + "_" + conversation_num
            print(qid, query)
            run_res = pipeline.retrieve(query, history=history, qid=qid)
            history.append(query)
            runs[qid] = run_res
            print("query {} runtime is:{} sec".format(qid, time.time() - query_start_time))
        for rewriter in rewriters:
            if isinstance(rewriter, T5Rewriter):
                print("reset history")
                rewriter.reset_history()
        print("session {} runtime is:{} sec".format(session_num, time.time() - start_time))
    run_output_file = "{}/{}_run.txt".format(args.output_dir, args.run_name)
    output_run_file(run_output_file, runs)


if __name__ == "__main__":
    # tf.debugging.set_log_device_placement(True)
    args = parse_args()
    print(args)
    if 'tpu' in args:
        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=args.tpu)
        tf.config.experimental_connect_to_cluster(resolver)
        tf.tpu.experimental.initialize_tpu_system(resolver)
        print("All devices: ", tf.config.list_logical_devices('TPU'))
    args_output_path = "{}/{}_config.json".format(args.output_dir, args.run_name)
    with open(args_output_path, 'w') as f:
        json.dump(vars(args), f, indent=True)
    k1 = args.k1
    b = args.b
    count = args.count
    searcher = SimpleSearcher.from_prebuilt_index("cast2019")
    searcher.set_bm25(k1, b)
    rewriters = []
    if 'add_prev_utter' in args and args.add_prev_utter:
        rewriters.append(PrevUtteranceRewriter())
    if 'add_first_utter' in args and args.add_first_utter:
        rewriters.append(FirstUtteranceRewriter())
    if 'add_all_history' in args and args.add_all_history:
        rewriters.append(AllHistoryRewriter())
    if 'separate_utterances_rewriter' in args and args.separate_utterances_rewriter:
        rewriters.append(SeparateUtterancesRewriter())
    if 'T5_rewriter' in args and args.T5_rewriter:
        rewriters.append(T5Rewriter(model_str=args.T5_model_str,num_queries_generated=args.T5_num_queries_generated))
    if 'quretec' in args and args.quretec:
        rewriters.append(QuReTeCRewriter(args.quretec_model_path))
    if 'queries_rewrites_path' in args:
        rewriters.append(FileRewriter(args.queries_rewrites_path))

    reranker = build_bert_reranker2(batch_size=args.reranker_batch_size) if args.rerank else None
    pipeline = Pipeline(searcher, rewriters, count, reranker)
    run_exp(args, pipeline)
