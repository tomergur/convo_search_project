import argparse
import json
import time

from pyserini.search import SimpleSearcher
import tensorflow as tf

from convo_search_project.pipeline import Pipeline
from convo_search_project.reranker import BertReranker
# TODO: delete
from convo_search_project.mono_bert import MonoBERT

from convo_search_project.rewriters import create_rewriter,HqeRewriter

def parse_args():
    parser = argparse.ArgumentParser(argument_default=argparse.SUPPRESS,
                                     description='running retrieval pipeline for our convo search project')
    parser.add_argument('--run_name', required=True,
                        help='the name of the current run. will be the name of the output files')
    parser.add_argument('--input_queries_file', required=True, help='input queries file')
    parser.add_argument('--output_dir', required=True, help='output dir for the run')
    parser.add_argument('--count', type=int, default=1000,
                        help="number of result retrieved in the first stage retrieval")
    parser.add_argument("--log_queries", action='store_true', default=True,
                        help="log the queries to a file in the output dir ")
    parser.add_argument('--tpu', help='name of tpu device')

    parser.add_argument("--use_manual_run", action='store_true', default=False,
                        help='use the manually rewritten queries')

    # Parameters for BM25.
    parser.add_argument('--k1', type=float, default=0.82, help='BM25 k1 parameter')
    parser.add_argument('--b', type=float, default=0.68, help='BM25 b parameter')

    #rewriter params
    parser.add_argument("--rewriters",nargs="+",default=[],help='list of rewriters')
    parser.add_argument("--second_stage_rewriters",nargs="+",
                        help="list of rewriters for the second stage retrieval")
    # T5 rewriter
    parser.add_argument('--T5_model_str', help='t5 rewriter model str')
    parser.add_argument('--T5_num_queries_generated', type=int, help='number of queries generated by the t5 model')
    parser.add_argument('--T5_rewriter_context_window', type=int, help="limit the context window of the t5 rewriter")
    parser.add_argument('--T5_rewriter_selected_query_rank', type=int,
                        help="start using the t5 output from an offset and not the best query")

    parser.add_argument('--T5_rewriter_sliding_window_fusion', action='store_true',
                        help='use sliding window fusion mode')
    # file rewriter
    parser.add_argument('--queries_rewrites_path', help='path to query rewrites cached in csv')

    # QuReTeC parameters
    parser.add_argument('--quretec_model_path', help='model path for QuReTec')

    # parameters for reranker
    parser.add_argument('--rerank', action='store_true', default=False, help='rerank BM25 output using BERT')
    parser.add_argument('--reranker_batch_size', type=int, default=32, help='reranker batch size for inference')
    # parser.add_argument('--reranker_device', default='cuda', help='reranker device to use')

    # Return args
    args = parser.parse_args()
    return args


def output_run_file(output_path, runs):
    print("write output", output_path)
    with open(output_path, "w") as f:
        for qid, run_res in runs.items():
            for rank, doc in enumerate(run_res):
                docno = doc.docid
                score = doc.score
                f.write("{}\tQ0\t{}\t{}\t{}\t{}\n".format(qid, docno, rank + 1, score, "convo"))


def output_queries_file(output_path, quries_dict):
    with open(output_path, "w") as f:
        json.dump(quries_dict, f, indent=True)


def build_bert_reranker(
        name_or_path: str = "castorini/monobert-large-msmarco-finetune-only", batch_size=32):
    """Returns a BERT reranker using the provided model name or path to load from"""
    model = BertReranker.get_model(name_or_path, from_pt=True)
    tokenizer = BertReranker.get_tokenizer(name_or_path)
    return BertReranker(model, tokenizer, batch_size=batch_size)


def build_bert_reranker2(
        name_or_path: str = "castorini/monobert-large-msmarco-finetune-only",
        batch_size:int =32,device: str = 'cuda'):
    """Returns a BERT reranker using the provided model name or path to load from"""
    model = MonoBERT.get_model(name_or_path, device=device)
    tokenizer = MonoBERT.get_tokenizer(name_or_path)
    return MonoBERT(model, tokenizer,batch_size)


def run_exp(args, pipeline):
    input_queries_file = args.input_queries_file
    query_field = "manual_rewritten_utterance" if args.use_manual_run else "raw_utterance"
    with open(input_queries_file) as json_file:
        data = json.load(json_file)
    runs = {}
    queries_dict = {}
    for session in data:
        start_time = time.time()
        session_num = str(session["number"])
        history = []
        for turn_id, conversations in enumerate(session["turn"]):
            query_start_time = time.time()
            query = conversations[query_field]
            conversation_num = str(conversations["number"])
            qid = session_num + "_" + conversation_num
            print(qid, query)
            if args.log_queries:
                run_res, query_dict = pipeline.retrieve(query, history=history, qid=qid)
                queries_dict[qid] = query_dict
            else:
                run_res = pipeline.retrieve(query, history=history, qid=qid)
            history.append(query)
            runs[qid] = run_res
            print("query {} runtime is:{} sec".format(qid, time.time() - query_start_time))
        for rewriter in rewriters:
            if isinstance(rewriter, HqeRewriter):
                print("reset history")
                rewriter.reset_history()
        print("session {} runtime is:{} sec".format(session_num, time.time() - start_time))
    run_output_file = "{}/{}_run.txt".format(args.output_dir, args.run_name)
    output_run_file(run_output_file, runs)
    if args.log_queries:
        queries_output_file = "{}/{}_queries.txt".format(args.output_dir, args.run_name)
        output_queries_file(queries_output_file, queries_dict)


if __name__ == "__main__":
    # tf.debugging.set_log_device_placement(True)
    args = parse_args()
    print(args)
    if 'tpu' in args:
        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=args.tpu)
        tf.config.experimental_connect_to_cluster(resolver)
        tf.tpu.experimental.initialize_tpu_system(resolver)
        print("All devices: ", tf.config.list_logical_devices('TPU'))
    args_output_path = "{}/{}_config.json".format(args.output_dir, args.run_name)
    with open(args_output_path, 'w') as f:
        json.dump(vars(args), f, indent=True)
    k1 = args.k1
    b = args.b
    count = args.count
    searcher = SimpleSearcher.from_prebuilt_index("cast2019")
    searcher.set_bm25(k1, b)
    rewriters = []
    for rewriter_name in args.rewriters:
        rewriters.append(create_rewriter(rewriter_name,args))
    second_stage_rewriters = None
    if 'second_stage_rewriters' in args:
        second_stage_rewriters=[]
        for rewriter_name in args.second_stage_rewriters:
            second_stage_rewriters.append(create_rewriter(rewriter_name, args))

    reranker = build_bert_reranker(batch_size=args.reranker_batch_size) if args.rerank else None
    pipeline = Pipeline(searcher, rewriters, count, reranker, second_stage_rewriters, args.log_queries)
    run_exp(args, pipeline)
